{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant librairies and python files\n",
    "import os\n",
    "from math import isnan\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "os.chdir('../utils')\n",
    "import utils_correlations\n",
    "import utils_correlation_activations\n",
    "import utils_evaluation\n",
    "import utils_xgboost\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "data_dir = \"../data/raw_in/\"\n",
    "file_name = \"Risques 2/data_set_challenge.csv\"\n",
    "mapping_name = \"Risques 2/final_mapping_candidat.csv\"\n",
    "\n",
    "df = pd.read_csv(os.path.join(data_dir, file_name), index_col=0)\n",
    "mapping = pd.read_csv(os.path.join(data_dir, mapping_name))\n",
    "\n",
    "# --- step 1: identify the different types of series\n",
    "df.columns = [str(typ) + \"_\" + str(col) for col, typ in zip(df.columns, mapping.Type)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the functions from utils_correlation to compute the correlation coefficients between the series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_rates_df = utils_correlations.get_growth_rates_df(df)\n",
    "corr_df = utils_correlations.get_correlations_df(df)\n",
    "corr_df_activated = utils_correlations.activate_correlations(corr_df,lambda x: utils_correlation_activations.truncate_below_threshold(x))\n",
    "corr_df_activated.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each time series, we will use the other time series from the dataframe to build the features for the model. for each model, we only use the time series with a correlation coefficient higher than 0.7 with the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dictionnary holding for each TS the other TS that will be used for training the model\n",
    "feature_map = {}\n",
    "for i, row in corr_df_activated.iterrows():\n",
    "    features = []\n",
    "    for col in row.index:\n",
    "        if (row[col] < 1 and abs(row[col])>0.7):\n",
    "            features.append(col)\n",
    "    feature_map[i] = features\n",
    "\n",
    "# Histogram of nb of feature per model\n",
    "\n",
    "nb_feature = [len(x) for x in feature_map.values()]\n",
    "n, bins, patches = plt.hist(x=nb_feature, bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('number of features for regression')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('number of features for regression')\n",
    "plt.text(23, 45, r'$\\mu=15, b=3$')\n",
    "maxfreq = n.max()\n",
    "# Set a clean upper y-axis limit.\n",
    "plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of series without a feature: (because no TS is correlated enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "\n",
    "for x in nb_feature:\n",
    "    if x == 0:\n",
    "        tot = tot + 1\n",
    "tot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each series we will fill in the missing values by using the predicytions of a model that uses only the relevant features from the feature map above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate our model we first need to get a valid evualuation set. We then fill in the missing values by building a model for every TS except the 302 ones without features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full,df_miss = utils_preprocessing.get_evaluation_set(df.reset_index().drop(\"Date\", axis=1), method=\"linear\")\n",
    "antoine_df = df_full.copy()\n",
    "\n",
    "k=0\n",
    "\n",
    "for x in feature_map.keys():\n",
    "    k = k+1\n",
    "    antoine_df[x] = utils_xgboost.model_for_series(x, feature_map[x], df_miss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We impute the values for the 302 TS left by linear interpolation (ie same method as benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = antoine_df.isnull()\n",
    "\n",
    "for val, idx in antoine_df.apply(pd.Series.first_valid_index).items():\n",
    "    mask.loc[idx, val] = True\n",
    "\n",
    "for val, idx in antoine_df.apply(pd.Series.last_valid_index).items():\n",
    "    mask.loc[idx, val] = True\n",
    "\n",
    "ix = [\n",
    "    (row, col)\n",
    "    for row, col in zip(\n",
    "        np.where(mask == False)[0],\n",
    "        np.where(mask == False)[1])\n",
    "]\n",
    "\n",
    "\n",
    "# --- impute the missing data\n",
    "antoine_dff = antoine_df.interpolate(\n",
    "    method=\"linear\", limit=None, limit_direction=\"forward\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a benchmark prediction by imputing the missing values on the original evaluation set by linear interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_miss_bench = df_miss.copy()\n",
    "\n",
    "mask = df_miss_bench.isnull()\n",
    "for val, idx in df_miss_bench.apply(pd.Series.first_valid_index).items():\n",
    "    mask.loc[idx, val] = True\n",
    "\n",
    "for val, idx in df_miss_bench.apply(pd.Series.last_valid_index).items():\n",
    "    mask.loc[idx, val] = True\n",
    "\n",
    "ix = [\n",
    "    (row, col)\n",
    "    for row, col in zip(\n",
    "        np.where(mask == False)[0],\n",
    "        np.where(mask == False)[1])\n",
    "]\n",
    "\n",
    "\n",
    "# --- impute the missing data\n",
    "df_miss_bench_res = df_miss_bench.interpolate(\n",
    "    method=\"linear\", limit=None, limit_direction=\"forward\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the evaluation methods to get the NRMSE.\n",
    "\n",
    "bench_dict, b, c = utils_evaluation.eval_imputation(\n",
    "    df_full, df_miss_bench_res, df_miss)\n",
    "print(b)\n",
    "\n",
    "\n",
    "xgb_dict, e, f = utils_evaluation.eval_imputation(\n",
    "    df_full, antoine_dff, df_miss)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we graph the NRMSE improvement of the xgboost method for the series with an improvement of at least 35% compared to benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compar = {}\n",
    "for key in bench_dict.keys():\n",
    "    compar[key] = ((bench_dict[key][0] - xgb_dict[key][0])/bench_dict[key][0])*100.0\n",
    "x = 0\n",
    "better_xgb = []\n",
    "for k in compar.keys():\n",
    "    if compar[k] > 35.0:\n",
    "        better_xgb.append((k, compar[k]))\n",
    "\n",
    "better_xgb.sort(key=itemgetter(1))\n",
    "\n",
    "y = [t[1] for t in better_xgb]\n",
    "xx = [t[0] for t in better_xgb]\n",
    "x = [t.split('_')[0] + ' ' + t.split('_')[len(t.split('_'))-1] for t in xx]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "MEDIUM_SIZE = 14\n",
    "SMALL_SIZE = 10\n",
    "\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('NRMSE improvment over Benchmark')\n",
    "plt.xlabel('Series')\n",
    "plt.ylabel('Improvement over Benchmark (%)')\n",
    "plt.rc('axes', titlesize=18)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=16)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=10)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=14)\n",
    "ax.bar(x, y)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
